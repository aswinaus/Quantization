{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aswinaus/Quantization/blob/main/CPU_Quantizing_LLMs_and_inferencing_Quantized_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBNz1xJYs5OJ"
      },
      "source": [
        "**This workbook covers following topics:**\n",
        "- What is Quantization\n",
        "- How to Quantize a model\n",
        "- How to Inference from quantized model\n",
        "\n",
        "**Quantization techniques covered in this notebook:**\n",
        "- llama.ccp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGwBBwWTw9-a"
      },
      "source": [
        "# **Intro to Quantization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pgPXi_ntfTf"
      },
      "source": [
        "***What is quantization of Large language model?***\n",
        "\n",
        "Quantization of Large Language Models (LLMs) is a technique used to reduce the computational and memory requirements of these models by converting their weights and activations from a high-precision 32-bit floating-point representation to a lower-precision format such as 8-bit or 4-bit integers. This process allows LLMs to be more efficiently run on hardware with limited computational resources, including mobile and IoT devices, without significantly compromising LLMâ€™s performance or accuracy.\n",
        "\n",
        "***What are the benefits of quantization in large language models***\n",
        "- Reduced Model Size / Memory Footprint\n",
        "- Faster Inference Speed / Increased Efficiency\n",
        "- Lower Power Consumption / Energy Efficiency - Suitable for mobile devices\n",
        "- Model Compression and Portability\n",
        "\n",
        "***What are different quantization techniques?***\n",
        "- Post-Training Quantization (PTQ)\n",
        "- Quantization-Aware Training (QAT)\n",
        "- Activation-Aware Weight Quantization (AWQ)\n",
        "- NF4 Quantization - BitsAndBytes\n",
        "- etc.\n",
        "\n",
        "***Different Options for Quantization:***\n",
        "- 16-bit (Float16)\n",
        "- 8-bit (Int8): for deploying models on edge devices or situations where computational resources are limited\n",
        "- 4-bit: Useful for extremely resource-constrained environments\n",
        "- 1-bit (Binary)\n",
        "- NF4 (4bit-NormalFloat): A specialized 4-bit format designed to efficiently represent a larger bit datatype. It includes steps like normalization, quantization, and dequantization to efficiently represent original 32-bit weights.Suitable for applications requiring a balance between model size reduction and maintaining higher accuracy than traditional 4-bit quantization.\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtbTyemDwFTU"
      },
      "source": [
        "--------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFC55vIYwWC3"
      },
      "source": [
        "# **Quantize models using GGUF and llama.cpp**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4seqb87BjU9Q"
      },
      "source": [
        "\n",
        "\n",
        "Useful links:\n",
        "- llama.cpp GitHub repo: [llama.cpp github repo](https://github.com/ggerganov/llama.cpp)\n",
        "- llama-cpp-python GitHub repo: https://github.com/abetlen/llama-cpp-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epG7RaodkMu_"
      },
      "source": [
        "*   **q2_k:** Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for the other tensors.\n",
        "*   **q3_k_l:** Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\n",
        "*   **q3_k_m:** Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\n",
        "*   **q3_k_s:** Uses Q3_K for all tensors\n",
        "*   **q4_0:** Original quant method, 4-bit.\n",
        "*   **q4_1:** Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models.\n",
        "*   **q4_k_m:** Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K\n",
        "*   **q4_k_s:** Uses Q4_K for all tensors\n",
        "*   **q5_0:** Higher accuracy, higher resource usage and slower inference.\n",
        "*   **q5_1:** Even higher accuracy, resource usage and slower inference.\n",
        "*   **q5_k_m:** Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K\n",
        "*   **q5_k_s:** Uses Q5_K for all tensors\n",
        "*   **q6_k:** Uses Q8_K for all tensors\n",
        "*   **q8_0:** Almost indistinguishable from float16. High resource use and slow. Not recommended for most users.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KACkLgRsPTVr"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "!cd llama.cpp && cmake -B build && cmake --build build --config Release"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3HAjnKmjQgf"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata, drive\n",
        "import torch\n",
        "import os\n",
        "from torch import bfloat16\n",
        "from huggingface_hub import login, HfApi, create_repo\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r llama.cpp/requirements.txt"
      ],
      "metadata": {
        "id": "SFJBntt4_8Kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --force-reinstall torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "id": "QjVsvtCv9a8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1VyFF-JNVpP",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# !cd llama.cpp && git pull && make clean && LLAMA_CUBLAS=1 make\n",
        "# !pip install -r llama.cpp/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cr8LdINzBFUU"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RC8DQqTBjQje"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from google.colab import userdata\n",
        "# Set the OpenAI API key as an environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] =  userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "HF_TOKEN = userdata.get('HUGGING_FACE_TOKEN')\n",
        "# Import the login function from huggingface_hub\n",
        "from huggingface_hub import login, HfApi\n",
        "login(token=HF_TOKEN)\n",
        "api = HfApi(token=HF_TOKEN)\n",
        "username = api.whoami()['name']\n",
        "print(username)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtXwYlcpkV0R"
      },
      "source": [
        "**Quantize meta-llama/Meta-Llama-3-8B-Instruct**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXjUphkzjQma"
      },
      "outputs": [],
      "source": [
        "# Define the model ID for the desired model\n",
        "model_id_llama = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "quantization_methods = [\"q5_k_m\", \"q4_k_m\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSSgSBVIjQpM"
      },
      "outputs": [],
      "source": [
        "model_name =  model_id_llama.split(\"/\")[-1]\n",
        "print(model_name)\n",
        "quant_name =  model_id_llama.split(\"/\")[-1] + \"-GGUF\"\n",
        "print(quant_name)\n",
        "quant_repo_id = f\"{username}/{quant_name}\"\n",
        "print(quant_repo_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " git-lfs refers to Git Large File Storage, an extension for Git that is designed to handle large files more efficiently. Normally, Git stores the entire history of every file within a repository. For large files, this can lead to performance issues and storage bloat. Git LFS addresses this by storing large files outside of the main Git repository and replacing them with pointers within the repository."
      ],
      "metadata": {
        "id": "E7oApHrvb2UE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Brtkru7sjQsK"
      },
      "outputs": [],
      "source": [
        "!git-lfs install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wf6M1tRmjQvC"
      },
      "outputs": [],
      "source": [
        "# Download model\n",
        "!git clone https://{username}:{HF_TOKEN}@huggingface.co/{model_id_llama}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to fp16\n",
        "fp16 = f\"{model_name}/{model_name.lower()}.fp16.bin\"\n",
        "!python llama.cpp/convert_hf_to_gguf.py {model_name} --outtype f16 --outfile {fp16}\n",
        "\n",
        "# Quantize the model for each method in the QUANTIZATION_METHODS list\n",
        "for method in quantization_methods:\n",
        "    qtype = f\"{model_name}/{model_name.lower()}.{method.upper()}.gguf\"\n",
        "    !./llama.cpp/build/bin/llama-quantize {fp16} {qtype} {method}"
      ],
      "metadata": {
        "id": "Cgr5q3qUBmhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYMn36I1jQ0_"
      },
      "outputs": [],
      "source": [
        "# Create an empty repo\n",
        "api.create_repo(\n",
        "    repo_id = quant_repo_id,\n",
        "    repo_type=\"model\",\n",
        "    exist_ok=True,\n",
        "    token=HF_TOKEN,\n",
        "    private=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload gguf files\n",
        "api.upload_folder(\n",
        "    folder_path=model_name,\n",
        "    repo_id=quant_repo_id,\n",
        "    token=HF_TOKEN\n",
        ")"
      ],
      "metadata": {
        "id": "yb3w4mBpUgHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJj6Ck9zk5JV"
      },
      "source": [
        "**Inferencing GGUF type models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfgJ1SvolNg6"
      },
      "source": [
        "**using llama_cpp (recommended)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oZTujazfcY7"
      },
      "outputs": [],
      "source": [
        "!pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSCyK8_og3uC"
      },
      "outputs": [],
      "source": [
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZpBtyfhlFTO"
      },
      "outputs": [],
      "source": [
        "from llama_cpp import Llama\n",
        "import os\n",
        "import dotenv\n",
        "from huggingface_hub import login, HfApi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gEw_RkGjQ7r"
      },
      "outputs": [],
      "source": [
        "dotenv.load_dotenv()\n",
        "\n",
        "HF_TOKEN = os.environ.get(\"HUGGING_FACE_API_KEY\")\n",
        "login(token=HF_TOKEN)\n",
        "api = HfApi(token=HF_TOKEN)\n",
        "username = api.whoami()[\"name\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6uY0ZHLjQ-t"
      },
      "outputs": [],
      "source": [
        "repo_id = \"bigopot420/Meta-Llama-3-8B-Instruct-GGUF\"\n",
        "filename = \"meta-llama-3-8b-instruct.Q4_K_M.gguf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2Qzr5O1jRBx"
      },
      "outputs": [],
      "source": [
        "prompt = \"Tell me a funny joke about Large Language Models meeting a Blackhole in an intergalactic Bar.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAAWi8r-jREn"
      },
      "outputs": [],
      "source": [
        "llm = Llama.from_pretrained(\n",
        "    repo_id=repo_id,\n",
        "    filename=filename,\n",
        "    verbose=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkLNFvr1jRHk"
      },
      "outputs": [],
      "source": [
        "llm_response = llm.create_chat_completion(\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    temperature=0.85,\n",
        "    top_p=0.8,\n",
        "    top_k=50,\n",
        "    repeat_penalty=1.01,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_response['choices'][0]['message']['content']"
      ],
      "metadata": {
        "id": "g8L4Vz05d3zL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}