{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "name": "mistral_quant_awq_load_local.ipynb",
      "gpuType": "A100",
      "mount_file_id": "1ji8P6NnoigiwFZXxJBTo0lfKnj5QBOYj",
      "authorship_tag": "ABX9TyN9qPB/8DmjXJSYYwXwqA1y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aswinaus/Quantization/blob/main/Load_Frozen_Model_SemanticIndex_Results.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**: Code in this notebook prepares - local_model.to(device) loads all the model's parameters and buffers to the specified runtime and fine-tunes a large language model for a specific task (likely related to Income tax statistics given the dataset) to improve its performance on that task."
      ],
      "metadata": {
        "id": "2rMyb2pp1q4i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jbxi59eeGs7b"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers torch accelerate langchain langchain_huggingface datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code is essentially forcing Python to always use \"UTF-8\" as the preferred encoding, regardless of the user's actual system settings. UTF-8 is a widely used encoding that can represent a vast range of characters from different languages. By enforcing UTF-8, you can help ensure that your code works consistently across different platforms and avoids encoding-related errors. It's a common practice for improving compatibility and preventing issues with text handling in Python programs."
      ],
      "metadata": {
        "id": "vpCeli-FI8me"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "59dUt6OwIvGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "c5-4gG8tIrFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "HUGGING_FACE_TOKEN = userdata.get('HUGGING_FACE_TOKEN')"
      ],
      "metadata": {
        "id": "UGGq_MH0LK3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token $HUGGING_FACE_TOKEN"
      ],
      "metadata": {
        "id": "6aIoujSHLcnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Download Data\n",
        "data_dir = '/content/drive/MyDrive'"
      ],
      "metadata": {
        "id": "R1y8bIH8PmnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import transformers\n",
        "import torch\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from threading import Thread"
      ],
      "metadata": {
        "id": "ML6AIgxOLraR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The nvidia-smi command is a utility provided by NVIDIA to query and display information about your NVIDIA GPU(s) (Graphics Processing Unit). This includes things like:\n",
        "\n",
        "GPU model and name\n",
        "Driver version\n",
        "GPU utilization\n",
        "Memory usage\n",
        "Temperature\n",
        "Power consumption\n",
        "Processes running on the GPU"
      ],
      "metadata": {
        "id": "Im62RU0RMXkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "GHtMY71ZL6tH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def wrap_text(text, width=90): #preserve_newlines\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text"
      ],
      "metadata": {
        "id": "58tu7qi6N1ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autoawq\n",
        "from awq import AutoAWQForCausalLM\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "mNvhFHRyRhZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple, Optional, Union, Dict, Any\n",
        "from transformers import PreTrainedModel, AutoModel, AutoTokenizer, AutoConfig\n",
        "from transformers.tokenization_utils_base import PreTrainedTokenizerBase"
      ],
      "metadata": {
        "id": "eF2xmVtIp14B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_dir = '/content/drive/MyDrive' # Input a data dir path from your mounted Google Drive"
      ],
      "metadata": {
        "id": "XpQJkOZdmeZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61a46bc1"
      },
      "source": [
        "!pip install --upgrade autoawq transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quant_path = f\"/{data_dir}/LLMs/Mistral/Mistral-Small-24B-Instruct-2501\""
      ],
      "metadata": {
        "id": "w3Uz7PsddoAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "local_model_path = quant_path\n",
        "local_tokenizer = AutoTokenizer.from_pretrained(quant_path)\n",
        "local_model = AutoAWQForCausalLM.from_pretrained(quant_path, low_cpu_mem_usage=True)"
      ],
      "metadata": {
        "id": "vaNKVuYhdrK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "local_model.to(device)"
      ],
      "metadata": {
        "id": "khHSPisIlgOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "# Replace with your actual access token\n",
        "from google.colab import userdata\n",
        "GRAPH_TOKEN = userdata.get('GRAPH_TOKEN')\n",
        "access_token = GRAPH_TOKEN\n",
        "\n",
        "url = \"https://graph.microsoft.com/beta/copilot/retrieval\"\n",
        "\n",
        "headers = {\n",
        "  \"Authorization\": f\"Bearer {access_token}\",\n",
        "  \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "request_body = {\n",
        "  \"queryString\": \"Please get me information about how many EYI MyDocs workspaces contains document about Netherlands workspace\",\n",
        "  \"dataSource\": \"sharePoint\",\n",
        "  \"resourceMetadata\": [\n",
        "    \"title\",\n",
        "    \"author\"\n",
        "  ],\n",
        "  \"maximumNumberOfResults\": \"10\"\n",
        "}\n",
        "\n",
        "response = requests.post(url, headers=headers, data=json.dumps(request_body))\n",
        "\n",
        "if response.status_code == 200:\n",
        "  data = response.json()\n",
        "  print(\"API Call Successful:\")\n",
        "  print(json.dumps(data, indent=2))\n",
        "else:\n",
        "  print(f\"API Call Failed with status code: {response.status_code}\")\n",
        "  #print(response.text)"
      ],
      "metadata": {
        "id": "BRzS3IdKkOcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if response.status_code == 200:\n",
        "  data = response.json()\n",
        "  print(\"API Call Successful:\")\n",
        "\n",
        "  # Rerank the results based on relevance score\n",
        "  if \"retrievalHits\" in data:\n",
        "    reranked_hits = sorted(data[\"retrievalHits\"], key=lambda x: x.get(\"relevanceScore\", 0), reverse=True)\n",
        "    data[\"retrievalHits\"] = reranked_hits\n",
        "    print(\"Results reranked by relevance score.\")\n",
        "\n",
        "  print(json.dumps(data, indent=2))\n",
        "else:\n",
        "  print(f\"API Call Failed with status code: {response.status_code}\")\n",
        "  #print(response.text)"
      ],
      "metadata": {
        "id": "qCB8LQO0k7ZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "local_model.to(device) moves all the model's parameters and buffers to the specified device (in this case, device, which is set to 'cuda' if a GPU is available). Deep learning models often have a large number of parameters and require significant computational power. GPUs are designed for parallel processing and can significantly speed up the training and inference of deep learning models. By moving the model to the GPU, you leverage its computational capabilities for faster execution."
      ],
      "metadata": {
        "id": "_jGUsY3gQz-s"
      }
    }
  ]
}