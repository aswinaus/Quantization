{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "name": "mistral_quant_awq_load_local.ipynb",
      "gpuType": "A100",
      "mount_file_id": "1ji8P6NnoigiwFZXxJBTo0lfKnj5QBOYj",
      "authorship_tag": "ABX9TyOHU1szyKLZhvgUItBTGJ8j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aswinaus/Quantization/blob/main/Load_Frozen_Model_SemanticIndex_Results.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**: Code in this notebook prepares - local_model.to(device) loads all the model's parameters and buffers to the specified runtime and fine-tunes a large language model for a specific task (likely related to Income tax statistics given the dataset) to improve its performance on that task."
      ],
      "metadata": {
        "id": "2rMyb2pp1q4i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jbxi59eeGs7b"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers torch accelerate langchain langchain_huggingface datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code is essentially forcing Python to always use \"UTF-8\" as the preferred encoding, regardless of the user's actual system settings. UTF-8 is a widely used encoding that can represent a vast range of characters from different languages. By enforcing UTF-8, you can help ensure that your code works consistently across different platforms and avoids encoding-related errors. It's a common practice for improving compatibility and preventing issues with text handling in Python programs."
      ],
      "metadata": {
        "id": "vpCeli-FI8me"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "59dUt6OwIvGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "c5-4gG8tIrFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "HUGGING_FACE_TOKEN = userdata.get('HUGGING_FACE_TOKEN')"
      ],
      "metadata": {
        "id": "UGGq_MH0LK3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token $HUGGING_FACE_TOKEN"
      ],
      "metadata": {
        "id": "6aIoujSHLcnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Download Data\n",
        "data_dir = '/content/drive/MyDrive'"
      ],
      "metadata": {
        "id": "R1y8bIH8PmnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import transformers\n",
        "import torch\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from threading import Thread"
      ],
      "metadata": {
        "id": "ML6AIgxOLraR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The nvidia-smi command is a utility provided by NVIDIA to query and display information about your NVIDIA GPU(s) (Graphics Processing Unit). This includes things like:\n",
        "\n",
        "GPU model and name\n",
        "Driver version\n",
        "GPU utilization\n",
        "Memory usage\n",
        "Temperature\n",
        "Power consumption\n",
        "Processes running on the GPU"
      ],
      "metadata": {
        "id": "Im62RU0RMXkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "GHtMY71ZL6tH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def wrap_text(text, width=90): #preserve_newlines\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text"
      ],
      "metadata": {
        "id": "58tu7qi6N1ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autoawq\n",
        "from awq import AutoAWQForCausalLM\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "mNvhFHRyRhZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple, Optional, Union, Dict, Any\n",
        "from transformers import PreTrainedModel, AutoModel, AutoTokenizer, AutoConfig\n",
        "from transformers.tokenization_utils_base import PreTrainedTokenizerBase"
      ],
      "metadata": {
        "id": "eF2xmVtIp14B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_dir = '/content/drive/MyDrive' # Input a data dir path from your mounted Google Drive"
      ],
      "metadata": {
        "id": "XpQJkOZdmeZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61a46bc1"
      },
      "source": [
        "!pip install --upgrade autoawq transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quant_path = f\"/{data_dir}/LLMs/Mistral/Mistral-Small-24B-Instruct-2501\""
      ],
      "metadata": {
        "id": "w3Uz7PsddoAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "local_model_path = quant_path\n",
        "local_tokenizer = AutoTokenizer.from_pretrained(quant_path)\n",
        "local_model = AutoAWQForCausalLM.from_pretrained(quant_path, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16)"
      ],
      "metadata": {
        "id": "vaNKVuYhdrK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "local_model.to(device)"
      ],
      "metadata": {
        "id": "khHSPisIlgOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "# Replace with your actual access token\n",
        "from google.colab import userdata\n",
        "GRAPH_TOKEN = userdata.get('GRAPH_TOKEN')\n",
        "access_token = GRAPH_TOKEN\n",
        "\n",
        "url = \"https://graph.microsoft.com/beta/copilot/retrieval\"\n",
        "\n",
        "headers = {\n",
        "  \"Authorization\": f\"Bearer {access_token}\",\n",
        "  \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "request_body = {\n",
        "  \"queryString\": \"Please get me information about how many EYI MyDocs workspaces contains document about Netherlands workspace\",\n",
        "  \"dataSource\": \"sharePoint\",\n",
        "  \"resourceMetadata\": [\n",
        "    \"title\",\n",
        "    \"author\"\n",
        "  ],\n",
        "  \"maximumNumberOfResults\": \"10\"\n",
        "}\n",
        "\n",
        "response = requests.post(url, headers=headers, data=json.dumps(request_body))\n",
        "\n",
        "if response.status_code == 200:\n",
        "  data = response.json()\n",
        "  print(\"API Call Successful:\")\n",
        "  print(json.dumps(data, indent=2))\n",
        "else:\n",
        "  print(f\"API Call Failed with status code: {response.status_code}\")\n",
        "  #print(response.text)"
      ],
      "metadata": {
        "id": "BRzS3IdKkOcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if response.status_code == 200:\n",
        "  data = response.json()\n",
        "  print(\"API Call Successful:\")\n",
        "\n",
        "  # Rerank the results based on relevance score\n",
        "  if \"retrievalHits\" in data:\n",
        "    reranked_hits = sorted(data[\"retrievalHits\"], key=lambda x: x.get(\"relevanceScore\", 0), reverse=True)\n",
        "    data[\"retrievalHits\"] = reranked_hits\n",
        "    print(\"Results reranked by relevance score.\")\n",
        "\n",
        "  print(json.dumps(data, indent=2))\n",
        "else:\n",
        "  print(f\"API Call Failed with status code: {response.status_code}\")\n",
        "  #print(response.text)"
      ],
      "metadata": {
        "id": "qCB8LQO0k7ZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "local_model.to(device) moves all the model's parameters and buffers to the specified device (in this case, device, which is set to 'cuda' if a GPU is available). Deep learning models often have a large number of parameters and require significant computational power. GPUs are designed for parallel processing and can significantly speed up the training and inference of deep learning models. By moving the model to the GPU, you leverage its computational capabilities for faster execution."
      ],
      "metadata": {
        "id": "_jGUsY3gQz-s"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11802cab"
      },
      "source": [
        "**Extracting Information and Generating Response with the Model**\n",
        "\n",
        "The following code cells will process the reranked retrieval results and use the loaded Mistral model to generate a response based on the original query and the retrieved information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33287f4c"
      },
      "source": [
        "# 2. Extract and Format Results\n",
        "retrieval_text = \"\"\n",
        "if \"retrievalHits\" in data:\n",
        "    for hit in data[\"retrievalHits\"]:\n",
        "        if \"extracts\" in hit:\n",
        "            for extract in hit[\"extracts\"]:\n",
        "                retrieval_text += extract.get(\"text\", \"\") + \"\\n\\n\"\n",
        "\n",
        "print(\"Extracted text from retrieval hits:\")\n",
        "print(wrap_text(retrieval_text[:500] + \"...\")) # Print a snippet to avoid flooding the output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ad72e5e5"
      },
      "source": [
        "# 3. Prepare Prompt\n",
        "query_string = request_body.get(\"queryString\", \"Information\") # Get the original query\n",
        "\n",
        "# Determine maximum context length, subtracting some tokens for the query and response\n",
        "max_context_length = local_model.config.max_position_embeddings - 100 # Subtract some buffer\n",
        "if local_tokenizer.model_max_length > 0: # Use tokenizer max length if available and smaller\n",
        "     max_context_length = min(max_context_length, local_tokenizer.model_max_length - 100)\n",
        "\n",
        "\n",
        "# Truncate retrieval_text if it's too long\n",
        "encoded_retrieval_text = local_tokenizer.encode(retrieval_text, max_length=max_context_length, truncation=True)\n",
        "truncated_retrieval_text = local_tokenizer.decode(encoded_retrieval_text)\n",
        "\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"\"\"Use the following information to answer the query:\n",
        "\n",
        "{retrieval_info}\n",
        "\n",
        "Query: {query}\n",
        "\n",
        "Response:\"\"\"\n",
        ")\n",
        "\n",
        "prompt = prompt_template.format(retrieval_info=truncated_retrieval_text, query=query_string)\n",
        "\n",
        "print(\"\\nGenerated Prompt:\")\n",
        "print(wrap_text(prompt[:500] + \"...\")) # Print a snippet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e62a484"
      },
      "source": [
        "import transformers\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "# 4. Generate Response\n",
        "# Configure the pipeline for text generation\n",
        "pipe = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=local_model,\n",
        "    tokenizer=local_tokenizer,\n",
        "    max_new_tokens=512, # Adjust as needed\n",
        "    do_sample=True,\n",
        "    temperature=0.7, # Adjust as needed\n",
        "    top_p=0.95,     # Adjust as needed\n",
        "    no_repeat_ngram_size=2,\n",
        "    return_full_text=False, # Only return the generated text, not the prompt\n",
        "    pad_token_id=local_tokenizer.eos_token_id, # Set pad_token_id to eos_token_id\n",
        "    device=device # Explicitly set the device\n",
        ")\n",
        "\n",
        "# Create a HuggingFacePipeline object\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "# Generate the response\n",
        "response = llm.invoke(prompt)\n",
        "\n",
        "# 5. Display Response\n",
        "print(\"\\nGenerated Response:\")\n",
        "print(wrap_text(response))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaaf1359"
      },
      "source": [
        "# 4. Generate Response (Directly using model.generate)\n",
        "inputs = local_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Generate text\n",
        "outputs = local_model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=512, # Adjust as needed\n",
        "    do_sample=True,\n",
        "    temperature=0.7, # Adjust as needed\n",
        "    top_p=0.95,     # Adjust as needed\n",
        "    no_repeat_ngram_size=2,\n",
        "    pad_token_id=local_tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Decode the generated text\n",
        "response = local_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# 5. Display Response\n",
        "print(\"\\nGenerated Response:\")\n",
        "print(wrap_text(response))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}